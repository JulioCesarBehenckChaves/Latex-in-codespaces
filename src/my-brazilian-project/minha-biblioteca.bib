@article{Cadeddu2024,
   abstract = {In recent years, transformer-based models have emerged as powerful tools for natural language processing tasks, demonstrating remarkable performance in several domains. However, they still present significant limitations. These shortcomings become more noticeable when dealing with highly specific and complex concepts, particularly within the scientific domain. For example, transformer models have particular difficulties when processing scientific articles due to the domain-specific terminologies and sophisticated ideas often encountered in scientific literature. To overcome these challenges and further enhance the effectiveness of transformers in specific fields, researchers have turned their attention to the concept of knowledge injection. Knowledge injection is the process of incorporating outside knowledge into transformer models to improve their performance on certain tasks. In this paper, we present a comprehensive study of knowledge injection strategies for transformers within the scientific domain. Specifically, we provide a detailed overview and comparative assessment of four primary methodologies, evaluating their efficacy in the task of classifying scientific articles. For this purpose, we constructed a new benchmark including both 24K labelled papers and a knowledge graph of 9.2K triples describing pertinent research topics. We also developed a full codebase to easily re-implement all knowledge injection strategies in different domains. A formal evaluation indicates that the majority of the proposed knowledge injection methodologies significantly outperform the baseline established by Bidirectional Encoder Representations from Transformers.},
   author = {Andrea Cadeddu and Alessandro Chessa and Vincenzo De Leo and Gianni Fenu and Enrico Motta and Francesco Osborne and Diego Reforgiato Recupero and Angelo Salatino and Luca Secchi},
   doi = {10.1016/j.engappai.2024.108166},
   issn = {09521976},
   journal = {Engineering Applications of Artificial Intelligence},
   keywords = {BERT,Classification,Knowledge graphs,Knowledge injection,Large language models,Natural language processing,Transformers},
   month = {7},
   publisher = {Elsevier Ltd},
   title = {A comparative analysis of knowledge injection strategies for large language models in the scholarly domain},
   volume = {133},
   year = {2024}
}
@article{Gao2024,
   abstract = {The fitness evaluation mechanism (FEM) based on nondominated sorting may lead to slow convergence when solving large-scale many-objective optimization problems (LSMaOPs), because the number of comparisons will become extremely large with the increase of optimization objectives and iterations. To solve this problem, a novel FEM based on distance and cosine similarity (DCS) is proposed in this paper. In each iteration, DCS needs to generate an ideal point after normalizing all objective functions. DCS consists of two important components, i.e., the distance and cosine similarity. The distance similarity that mines the similar relationship between solutions and ideal point is calculated as the convergence measure, and the cosine similarity that reflects the uniformity of solution distribution is calculated as the diversity measure. Furthermore, DCS fuses the distance and cosine similarity into a comprehensive similarity to fully evaluate the quality of solutions. Both theoretical analysis and empirical results indicate that DCS has lower computational complexity than other state-of-the-art FEMs. To verify the performance of DCS in solving LSMaOPs, DCS and the competitors are respectively embedded in genetic algorithm, and then compared on 56 test instances with 5–15 objectives and 100–1000 decision variables. The experimental results show the effectiveness and superiority of DCS.},
   author = {Cong Gao and Wenfeng Li and Lijun He and Lingchong Zhong},
   doi = {10.1016/j.engappai.2024.108127},
   issn = {09521976},
   journal = {Engineering Applications of Artificial Intelligence},
   keywords = {Cosine similarity,Distance similarity,Fitness evaluation mechanism,Large-scale many-objective optimization},
   month = {7},
   publisher = {Elsevier Ltd},
   title = {A distance and cosine similarity-based fitness evaluation mechanism for large-scale many-objective optimization},
   volume = {133},
   year = {2024}
}
@techReport{Vaswani2017,
   abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
   author = {Ashish Vaswani and Google Brain and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N Gomez and Łukasz Kaiser and Illia Polosukhin},
   city = {Long Beach, CA, USA},
   institution = {31st Conference on Neural Information Processing Systems (NIPS 2017)},
   title = {Attention Is All You Need},
   year = {2017}
}
@techReport{Reimers2019,
   abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However , it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods. 1},
   author = {Nils Reimers and Iryna Gurevych},
   city = {Hong Kong, China},
   institution = {Association for Computational Linguistics},
   month = {11},
   pages = {3982-3992},
   title = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
   url = {https://github.com/UKPLab/},
   year = {2019}
}
@book{Brotman2025,
   abstract = {AI is going to change brand strategy and marketing forever. Are you ready?
What does the rapid rise and astonishing rate of improvement of AI mean for brands in the next five years? Listen to what OpenAI CEO Sam Altman told authors Adam Brotman and Andy Sack when he met them: "It will mean that 95 percent of what marketers use agencies, strategists, and creative professionals for today will easily, nearly instantly, and at almost no cost be handled by AI. No problem."
Upon hearing that astonishing statement, the authors began a journey of discovery to understand what the transition to an AI first world would mean. You'll hear from a who's who of tech visionaries who spoke with the authors, including Altman himself, Bill Gates, and Reid Hoffman, sharing how they're thinking of the transition to the new reality. You'll also hear from practitioners bold enough to be surfing this tidal wave of change, including one who audaciously mandated experimentation with AI for all his employees.
Brotman is the former chief digital officer at Starbucks, pivotal in the development of the coffee giant's mobile payment and loyalty programs. Sack is a legendary tech visionary and former adviser to Microsoft CEO Satya Nadella. Together, they formed the strategic consultancy Forum3 to take on every aspect of the challenge of becoming an AI first organization, including how you think about the design of jobs, what skills you need to develop within your organization, what your customers will expect from your brands, and how you can achieve early wins. In the AI first arena, where almost anyone can build creatively engaging brands quickly and cheaply, you need to know how to compete.
It's time to get ready for a brand-new world. Start here.},
   author = {Adam Brotman and Andy Sack},
   pages = {-193},
   publisher = {Harvard Business Review Press},
   title = {AI First: The Playbook for a Future-Proof Business and Brand (English Edition)},
   year = {2025}
}
@book{Carter2025,
   abstract = {"It's exponential baby, the change that is upon us with artificial intelligence at the epicenter of innovation. It's now about "AI First" and not forgetting the Human touch! We're not facing a single disruptive force; we're amid a whirlwind of change. We all need to prepare to embrace the AI First mindset that's essential. Through transformative insights, this book will recalibrate your approach, embedding AI seamlessly into the DNA of your strategies while using the human element. Learn how to harness the power of AI and turn these unstoppable currents of change into tailwinds propelling you and your company forward."-- Provided by publisher. Foreword ix -- Chapter 1 Embracing the AI-First Era 1 -- Chapter 2 Exponential Baby! 21 -- Chapter 3 The Rise of Multimodal Learning Models 45 -- Chapter 4 The Experiential Age Unfolds 67 -- Chapter 5 Everything Is Being Digitally Twinned 93 -- Chapter 6 Tokenization of Everything 115 -- Chapter 7 The Convergence Concept 141 -- Chapter 8 Challenges Brought on by AI 165 -- Chapter 9 How to Become an AI-First Leader 203 -- Chapter 10 The Future Horizon: AI's Transformative Path 235 -- Appendix A: The AI Marketecture: A Practical Guide for Leaders 259 -- Appendix B: First Principles Thinking and Navigating Rapid Change 265 -- Notes 271 -- Acknowledgments 283 -- About the Author 285 -- Index 287.},
   author = {Sandy Carter},
   isbn = {1394189826},
   pages = {304},
   publisher = {John Wiley \& Sons, Inc.},
   title = {AI first, human always : embracing a new mindset for the era of superintelligence},
   year = {2025}
}
@book{jm3,
   abstract = {https://web.stanford.edu/~jurafsky/slp3/},
   author = {Daniel Jurafsky and James H. Martin},
   edition = {3rd},
   month = {8},
   pages = {1-622},
   publisher = {Online manuscript},
   title = {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models},
   year = {2025}
}
@inproceedings{Ma2025,
   abstract = {Large language models (LLMs) have demonstrated remarkable performance on question-answering (QA) tasks because of their superior capabilities in natural language understanding and generation. However, LLM-based QA struggles with complex QA tasks due to poor reasoning capacity, outdated knowledge, and hallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs) for QA to address the above challenges. In this survey, we propose a new structured taxonomy that categorizes the methodology of synthesizing LLMs and KGs for QA according to the categories of QA and the KG's role when integrating with LLMs. We systematically survey state-of-the-art methods in synthesizing LLMs and KGs for QA and compare and analyze these approaches in terms of strength, limitations, and KG requirements. We then align the approaches with QA and discuss how these approaches address the main challenges of different complex QA. Finally, we summarize the advancements, evaluation metrics, and benchmark datasets and highlight open challenges and opportunities.},
   author = {Chuangtao Ma and Yongrui Chen and Tianxing Wu and Arijit Khan and Haofen Wang},
   booktitle = {Proceedings ofthe 2025 Conference on Empirical Methods in Natural Language Processing},
   month = {11},
   pages = {24590-24609},
   publisher = {Association for Computational Linguistics},
   title = {Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities},
   url = {https://github.com/machuangtao/LLM-KG4QA},
   year = {2025}
}
@book{Manning2009,
   author = {Christopher D. Manning and Prabhakar Raghavan and Hinrich Schütze},
   city = {Cambridge},
   edition = {Online edition},
   publisher = {Cambridge University Press},
   title = {An Introduction to Information Retrieval},
   year = {2009}
}
@inproceedings{Ovadia2024,
   abstract = {Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsu-pervised fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem.},
   author = {Oded Ovadia and Meni Brief and Moshik Mishaeli and Oren Elisha Microsoft},
   booktitle = {Proceedings ofthe 2024 Conference on Empirical Methods in Natural Language Processing},
   month = {11},
   pages = {237-250},
   publisher = {Association for Computational Linguistics},
   title = {Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs},
   year = {2024}
}
@inproceedings{Gomes2025,
   abstract = {Sentence encoder encode the semantics of their input, enabling key downstream applications such as classification, clustering, or retrieval. In this paper, we present Serafim, a family of open-source sentence encoders for Portuguese with various sizes, suited to different hardware/compute budgets. Each model exhibits state-of-the-art performance and is made openly available under a permissive license, allowing its use for both commercial and research purposes. Besides the sentence encoders, this paper contributes a systematic study and lessons learned concerning the selection criteria of learning objectives and parameters that support top-performing encoders.},
   author = {Luís Gomes and António Branco and João Silva and João Rodrigues and Rodrigo Santos},
   doi = {10.1007/978-3-031-73503-5_22},
   isbn = {9783031735028},
   issn = {16113349},
   booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) },
   keywords = {Large Language Model,Portuguese,Sentence encoder},
   pages = {267-279},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Open Sentence Embeddings for Portuguese with the Serafim PT* Encoders Family},
   volume = {14969 LNAI},
   year = {2025}
}

@article{Araujo2022,
  author = {Araujo, I. A.},
  title = {Elites jurídicas no Brasil: O caso do Tribunal de Segurança Nacional (1936-1945)},
  journal = {Conversas \& Controvérsias},
  volume = {9},
  number = {1},
  pages = {e41589},
  year = {2022},
  address = {[S. l.]}
}

@article{Beloch1978,
  author = {Beloch, I.},
  title = {Dicionário Histórico-Biográfico Brasileiro: dilemas na elaboração de um Dicionário de História Política},
  journal = {Revista de Ciência Política},
  address = {Rio de Janeiro},
  volume = {21},
  number = {3},
  pages = {29--52},
  month = {jul./set.},
  year = {1978}
}

@article{Berg2015,
  author = {Berg, O. and Bringhenti, T. and Cardoso, S.},
  title = {Entre Arena e MDB: análise das trajetórias dos remanescentes do Partido Libertador frente à ruptura democrática de 1964},
  journal = {Revista Paraná Eleitoral},
  address = {Curitiba},
  volume = {4},
  number = {3},
  pages = {473--496},
  year = {2015}
}

@incollection{Conniff1991,
  author = {Conniff, M.},
  title = {The national elite},
  booktitle = {Modern Brazil: elites and masses in historical perspective},
  editor = {Conniff, M. and McCann, F.},
  publisher = {University of Nebraska Press},
  year = {1991}
}

@incollection{Conniff2003,
  author = {Conniff, M.},
  title = {O DHBB e os brasilianistas},
  booktitle = {CPDOC 30 Anos},
  editor = {{FGV, E.}},
  publisher = {Editora FGV/CPDOC},
  address = {Rio de Janeiro},
  year = {2003}
}

@incollection{Costa2015,
  author = {Costa, L. D. and others},
  title = {O desenho e as fontes da pesquisa com elites parlamentares brasileiras no século XX},
  booktitle = {Como estudar elites},
  editor = {Perissinotto, R. and Codato, A.},
  publisher = {Ed. UFPR},
  address = {Curitiba},
  year = {2015},
  chapter = {1},
  pages = {15--30}
}

@article{Dominici2022,
  author = {Dominici, Lorenna},
  title = {Portavoces del mundo social: un estudio de la trayectoria de los parlamentarios brasileños que escriben sobre educación},
  journal = {Revista Agenda Política},
  address = {[S. l.]},
  volume = {9},
  number = {1},
  pages = {106--133},
  year = {2022}
}

@article{Grill2012,
  author = {Grill, I. G. and Reis, E. T.},
  title = {O que escrever quer dizer na política? Carreiras políticas e gêneros de produção escrita},
  journal = {Revista Pós de Ciências Sociais},
  volume = {09},
  number = {17},
  year = {2012}
}

@incollection{Heinz2011,
  author = {Heinz, F. M.},
  title = {Nota sobre o uso de anuários sociais do tipo Who’s Who em pesquisa prosopográfica},
  booktitle = {História social de elites},
  editor = {Heinz, F. M.},
  publisher = {Oikos},
  address = {São Leopoldo},
  year = {2011},
  pages = {154--165}
}

@phdthesis{Higuchi2021,
  author = {Higuchi, S.},
  title = {Extração automática de informações: uma leitura distante do Dicionário Histórico-Biográfico Brasileiro (DHBB)},
  school = {Pontifícia Universidade Católica do Rio de Janeiro, Departamento de Letras},
  year = {2021},
  type = {Tese (doutorado)}
}

@article{Paiva2019,
  author = {Paiva, M. J.},
  title = {Análise da forma de recrutamento das elites político-econômicas ministeriais de Castelo Branco a Lula},
  journal = {Perspectivas: Revista de Ciências Sociais},
  address = {São Paulo},
  volume = {53},
  pages = {81--101},
  month = {jan./jun.},
  year = {2019}
}

@inproceedings{Paiva2014,
  author = {Paiva, V. and Oliveira, D. A. B. and Rademaker, Alexandre and Higuchi, Suemi and Melo, G.},
  title = {Exploratory Information Extraction from a Historical Dictionary},
  booktitle = {Proceedings 1st Workshop On Digital Humanities and E-Science Associated With 10th IEEE International Conference On E-Science},
  address = {São Paulo},
  year = {2014}
}

@book{Perissinotto2015,
  editor = {Perissinotto, R. and Codato, A.},
  title = {Como estudar elites},
  publisher = {Ed. UFPR},
  address = {Curitiba},
  year = {2015}
}

@article{Ricci2024,
  author = {Ricci, P. and Arruda, L. R. V. and Massimo, L. and Zulini, J. P.},
  title = {A metamorfose do governo representativo no Brasil: o que muda com a Revolução de 1930?},
  journal = {Opinião Pública},
  address = {Campinas},
  volume = {30},
  pages = {1--35},
  year = {2024},
  note = {e30114}
}

@article{Schaefer2024,
  author = {Schaefer, B. M. and Barbosa, T. A. L.},
  title = {Quem são os líderes? Fragmentação partidária e a liderança de governo na democracia brasileira recente (1995-2016)},
  journal = {Missões: Revista de Ciências Humanas e Sociais},
  address = {[S. l.]},
  volume = {2},
  number = {1},
  pages = {1--23},
  year = {2024}
}

@article{Schlegel2024,
  author = {Schlegel, R.},
  title = {Interventores na transição pós-Estado Novo (1945-1947): o elo perdido da descentralização limitada?},
  journal = {Estudos Históricos},
  address = {Rio de Janeiro},
  volume = {37},
  number = {81},
  pages = {e20240104},
  year = {2024}
}

@incollection{Zulini2025,
  author = {Zulini, J. P.},
  title = {O estudo das elites políticas inspirado em PBL: engajando os estudantes por meio da produção de verbetes para o Dicionário Histórico-Biográfico Brasileiro (DHBB)},
  booktitle = {Ensino de ciências sociais e história: experiências do CPDOC},
  editor = {Castro, C. and Vannucchi, M. A. and Blank, T.},
  publisher = {FGV Editora},
  address = {Rio de Janeiro},
  edition = {1},
  year = {2025},
  pages = {31--72}
}

@incollection{Zulini2026,
  author = {Zulini, J. P. and Chaves, J. and Higuchi, S.},
  title = {O Dicionário Histórico-Biográfico Brasileiro no contexto das Humanidades Digitais},
  booktitle = {Humanidades Digitais: Experiências no CPDOC},
  editor = {Higuchi, S. and Marques, J. and Souza, R. R.},
  publisher = {Editora FGV},
  address = {Rio de Janeiro},
  year = {2026},
  note = {(previsto)}
}
